[
  {
    "objectID": "certifications.html",
    "href": "certifications.html",
    "title": "Certifications",
    "section": "",
    "text": "Over the past few years, I’ve actively engaged with a wide range of additional courses to build practical, industry-relevant skills in data analytics, AI, visualisation, and business software. These certifications represent more than just technical knowledge, they reflect my ongoing commitment to continuous learning and applying data-driven insights in real-world scenarios. From early foundations in Excel and Tableau to more advanced concepts in Python, SQL, and AI, each course has contributed to deepening my analytical thinking and problem-solving approach. Collectively, these certifications have given me a strong, well-rounded skill set that I’ve applied across academic projects, business cases, and technical implementations.\nHere are some selected certifications that represent a broad mix of my technical and analytical development:"
  },
  {
    "objectID": "certifications.html#excel",
    "href": "certifications.html#excel",
    "title": "Certifications",
    "section": "Excel",
    "text": "Excel\n\n\n\nFinancial Modelling"
  },
  {
    "objectID": "certifications.html#python",
    "href": "certifications.html#python",
    "title": "Certifications",
    "section": "Python",
    "text": "Python\n\n\n\nIntermediate Python"
  },
  {
    "objectID": "certifications.html#power-bi",
    "href": "certifications.html#power-bi",
    "title": "Certifications",
    "section": "Power BI",
    "text": "Power BI\n\n\n\nPower BI Dashboards"
  },
  {
    "objectID": "certifications.html#sql",
    "href": "certifications.html#sql",
    "title": "Certifications",
    "section": "SQL",
    "text": "SQL\n\n\n\nSQL"
  },
  {
    "objectID": "certifications.html#full-kubicle-certificate-list",
    "href": "certifications.html#full-kubicle-certificate-list",
    "title": "Certifications",
    "section": "Full Kubicle Certificate List",
    "text": "Full Kubicle Certificate List"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "David Byrne",
    "section": "",
    "text": "Welcome to my ePortfolio! 👋\nHere you’ll find a curated showcase of my projects and work over the last 4 years. This space highlights my skills, interests, and the journey I’ve taken. Feel free to explore, connect with me on LinkedIn, or reach out to me!\nLearn more about me →"
  },
  {
    "objectID": "projects/SQL/SQL_exam.html",
    "href": "projects/SQL/SQL_exam.html",
    "title": "SQL Exam Code",
    "section": "",
    "text": "-- Remove any old tables\nDROP TABLE IF EXISTS Orders;\nDROP TABLE IF EXISTS Products;\nDROP TABLE IF EXISTS Customers;\n\n-- Create a new database and switch into it\nCREATE DATABASE TechCie;\nUSE TechCie;"
  },
  {
    "objectID": "projects/SQL/SQL_exam.html#setup-database-creation",
    "href": "projects/SQL/SQL_exam.html#setup-database-creation",
    "title": "SQL Exam Code",
    "section": "",
    "text": "-- Remove any old tables\nDROP TABLE IF EXISTS Orders;\nDROP TABLE IF EXISTS Products;\nDROP TABLE IF EXISTS Customers;\n\n-- Create a new database and switch into it\nCREATE DATABASE TechCie;\nUSE TechCie;"
  },
  {
    "objectID": "projects/SQL/SQL_exam.html#table-definitions",
    "href": "projects/SQL/SQL_exam.html#table-definitions",
    "title": "SQL Exam Code",
    "section": "2. Table Definitions",
    "text": "2. Table Definitions\n-- Customers table\nCREATE TABLE Customers (\n  Customer_ID   INT PRIMARY KEY,\n  Customer_Name VARCHAR(20),\n  Email         VARCHAR(30)\n);\n\n-- Products table\nCREATE TABLE Products (\n  Product_ID   INT PRIMARY KEY,\n  Product_Name VARCHAR(20),\n  Price        FLOAT\n);\n\n-- Orders table, with foreign keys\nCREATE TABLE Orders (\n  Order_ID     INT PRIMARY KEY,\n  Customer_ID  INT,\n  Product_ID   INT,\n  Quantity     INT,\n  Order_Date   DATE,\n  FOREIGN KEY (Customer_ID) REFERENCES Customers(Customer_ID),\n  FOREIGN KEY (Product_ID)  REFERENCES Products(Product_ID)\n);"
  },
  {
    "objectID": "projects/SQL/SQL_exam.html#insert-sample-data",
    "href": "projects/SQL/SQL_exam.html#insert-sample-data",
    "title": "SQL Exam Code",
    "section": "3. Insert Sample Data",
    "text": "3. Insert Sample Data\n-- Populate Customers\nINSERT INTO Customers (Customer_ID, Customer_Name, Email)\nVALUES\n  (1, 'Alice Smith', 'alice@dcu.com'),\n  (2, 'Bob Johnson', 'bob@dcu.com'),\n  (3, 'Carol Davis', 'carol@ucd.com'),\n  (4, 'David Brown', 'david@tcd.com');\n\n-- Populate Products\nINSERT INTO Products (Product_ID, Product_Name, Price)\nVALUES\n  (101, 'Laptop',    1200),\n  (102, 'Smartphone', 800.5),\n  (103, 'Tablet',     400),\n  (104, 'Monitor',    250),\n  (105, 'Keyboard',    50.5);\n\n-- Populate Orders\nINSERT INTO Orders (Order_ID, Customer_ID, Product_ID, Quantity, Order_Date)\nVALUES\n  (1, 1, 101, 1, '2024-01-15'),\n  (2, 1, 105, 2, '2024-01-16'),\n  (3, 2, 102, 1, '2024-01-15'),\n  (4, 3, 103, 1, '2024-03-05'),\n  (5, 4, 104, 1, '2024-04-15'),\n  (6, 3, 105, 3, '2024-03-15');"
  },
  {
    "objectID": "projects/SQL/SQL_exam.html#explore-the-data",
    "href": "projects/SQL/SQL_exam.html#explore-the-data",
    "title": "SQL Exam Code",
    "section": "4. Explore the Data",
    "text": "4. Explore the Data\n\n4.2 Retrieve product names\nSELECT Product_Name\nFROM Products;\n\n\n4.3 Add new customer\nINSERT INTO Customers (Customer_ID, Customer_Name, Email)\nVALUES (5, 'Ewan Curren', 'ewan@dcu.com');\n\n\n4.4 Update customer email\nUPDATE Customers\nSET Email = 'david@dcu.com'\nWHERE Customer_ID = 4;\n\n\n4.5 Average quantity ordered\nSELECT AVG(Quantity) AS avg_quantity\nFROM Orders;\n\n\n4.6 Products costing more than 300\nSELECT Product_Name\nFROM Products\nWHERE Price &gt; 300;\n\n\n4.7 Customers who ordered on 2024‑01‑15\nSELECT c.Customer_Name\nFROM Customers AS c\nJOIN Orders    AS o\n  ON c.Customer_ID = o.Customer_ID\nWHERE o.Order_Date = '2024-01-15';\n\n\n4.8 Total quantity per customer after 2024‑01‑31\nSELECT c.Customer_Name,\n       SUM(o.Quantity) AS Total_Quantity\nFROM Orders    AS o\nJOIN Customers AS c\n  ON o.Customer_ID = c.Customer_ID\nWHERE o.Order_Date &gt; '2024-01-31'\nGROUP BY c.Customer_Name;\n\n\n4.9 Total quantity per customer (include those with no orders)\nSELECT c.Customer_Name,\n       COALESCE(SUM(o.Quantity), 0) AS Total_Quantity\nFROM Customers AS c\nLEFT JOIN Orders AS o\n  ON c.Customer_ID = o.Customer_ID\nGROUP BY c.Customer_Name\nORDER BY Total_Quantity DESC;\n\n\n4.10 Remove Bob Johnson entirely\nDELETE FROM Orders\nWHERE Customer_ID = 2;\n\nDELETE FROM Customers\nWHERE Customer_ID = 2;"
  },
  {
    "objectID": "projects/ned.html",
    "href": "projects/ned.html",
    "title": "New Enterprise Development",
    "section": "",
    "text": "Our product, Revive Grounds, is a sustainable skin scrub made using repurposed coffee grounds, and this module gave us the perfect framework to bring it to life. It has been one of the most engaging and practical experiences of my degree so far, offering a strong balance between theory and real-world application. From identifying a clear market opportunity to building investor-ready financials, the process challenged us to think creatively while remaining commercially focused. The Dragons’ Den pitch was a standout moment, helping us refine our communication and present with confidence to experienced professionals. I also gained a better appreciation for teamwork as we worked closely to develop our brand from the ground up. Overall, the module gave me a much deeper understanding of entrepreneurship and sustainability in business, and helped boost both my practical skills and personal confidence along the way."
  },
  {
    "objectID": "projects/ned.html#my-experience",
    "href": "projects/ned.html#my-experience",
    "title": "New Enterprise Development",
    "section": "",
    "text": "Our product, Revive Grounds, is a sustainable skin scrub made using repurposed coffee grounds, and this module gave us the perfect framework to bring it to life. It has been one of the most engaging and practical experiences of my degree so far, offering a strong balance between theory and real-world application. From identifying a clear market opportunity to building investor-ready financials, the process challenged us to think creatively while remaining commercially focused. The Dragons’ Den pitch was a standout moment, helping us refine our communication and present with confidence to experienced professionals. I also gained a better appreciation for teamwork as we worked closely to develop our brand from the ground up. Overall, the module gave me a much deeper understanding of entrepreneurship and sustainability in business, and helped boost both my practical skills and personal confidence along the way."
  },
  {
    "objectID": "projects/ml code/ml_code.html",
    "href": "projects/ml code/ml_code.html",
    "title": "Machine Learning Project Code",
    "section": "",
    "text": "# ---------------------------------------\n# 0. Import Libraries\n# ---------------------------------------\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay, PrecisionRecallDisplay,f1_score, accuracy_score\nfrom tabulate import tabulate \n\n\n# ---------------------------------------\n# 1. Load Dataset\n# ---------------------------------------\nfile = '/Users/davidbyrne/Documents/ML Research Paper/creditcard.csv'\ndata = pd.read_csv(file)\n\n\n# ---------------------------------------\n# 2. Exploratory Data Analysis (EDA)\n# ---------------------------------------\n\n#2.1 Inspect structure and missing values\nprint(\"Dataset preview:\")\nprint(data.head())\nprint(\"\\nDataset info:\")\nprint(data.info())\nprint(\"\\nMissing values by column:\")\nprint(data.isna().sum())  # ADDED: print missing values summary\n\nDataset preview:\n   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]\n\nDataset info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\nNone\n\nMissing values by column:\nTime      0\nV1        0\nV2        0\nV3        0\nV4        0\nV5        0\nV6        0\nV7        0\nV8        0\nV9        0\nV10       0\nV11       0\nV12       0\nV13       0\nV14       0\nV15       0\nV16       0\nV17       0\nV18       0\nV19       0\nV20       0\nV21       0\nV22       0\nV23       0\nV24       0\nV25       0\nV26       0\nV27       0\nV28       0\nAmount    0\nClass     0\ndtype: int64\n\n\n\n#2.2 Class imbalance overview\n#Get class counts\ndata.Class.value_counts()\nlabels=[\"Genuine\",\"Fraud\"]\nis_it_fraud = data[\"Class\"].value_counts().tolist()\nvalues = [is_it_fraud[0], is_it_fraud[1]]\n\n#Pie chart of class proportions\n\nplt.figure(figsize=(6, 6))\nplt.pie(values, labels=labels, autopct='%1.2f%%', startangle=0, colors=['skyblue', 'lightcoral'])\nplt.title(\"Fraud vs Genuine Transactions Pie Chart\", fontsize=15)\nplt.axis('equal') \nplt.show()\n\n#Bar chart of raw class counts\nplt.figure(figsize=(6,6))\nax = sns.countplot(x='Class',data=data,color=\"skyblue\")\nfor i in ax.containers:\n    ax.bar_label(i,)\n    \nplt.title(\"Fraud vs Genuine Transactions Bar Chart\", fontsize=15)\nplt.xlabel(\"Class (0 = Genuine, 1 = Fraud)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#2.3 Feature correlations\ncorr_matrix = data.corr()\n\n#Plot correlation heatmap\nplt.figure(figsize=(16, 12))\nsns.heatmap(corr_matrix, cmap='coolwarm', center=0, linewidths=0.5)\nplt.title(\"Correlation Matrix of All Features\", fontsize=15)\nplt.show()\n\n#Top 10 feature correlation\ntop_corr = corr_matrix['Class'].abs().sort_values(ascending=False).head(10)\nprint('Top 10 features by absolute correlation with Class:')\nprint(top_corr)\n\n\n\n\n\n\n\n\nTop 10 features by absolute correlation with Class:\nClass    1.000000\nV17      0.326481\nV14      0.302544\nV12      0.260593\nV10      0.216883\nV16      0.196539\nV3       0.192961\nV7       0.187257\nV11      0.154876\nV4       0.133447\nName: Class, dtype: float64\n\n\n\n# 2.4 Amount distribution analysis\n\n#Histogram\nplt.figure(figsize=(8, 5))\nsns.histplot(data['Amount'], bins=100, kde=True)\nplt.title(\"Distribution of Transaction Amounts\")\nplt.xlabel(\"Amount\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n#Boxplot\nsns.boxplot(x='Class', y='Amount', data=data)\nplt.title(\"Transaction Amount by Class (0 = Genuine, 1 = Fraud)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ---------------------------------------\n# 3. Data Preprocessing\n# ---------------------------------------\n\n#3.1 Remove duplicates\n\n#Check if any duplicates are frauds\nfraud_duplicates = data[data.duplicated() & (data['Class'] == 1)]\nprint(\"Fraud duplicates:\", len(fraud_duplicates))\n\nFraud duplicates: 19\n\n\n\n#Separate fraud and non-fraud cases\nfraud = data[data['Class'] == 1]\nnon_fraud = data[data['Class'] == 0]\n\n#Drop duplicates only in non-fraud cases\nnon_fraud = non_fraud.drop_duplicates()\n\n#Combine both sets back into a single DataFrame\ndata = pd.concat([fraud, non_fraud], ignore_index=True)\n\n#Confirm new shape and class balance\nprint(\"Data shape after cleaning:\", data.shape)\nprint(\"Class distribution after cleaning:\\n\", data['Class'].value_counts())\n\nData shape after cleaning: (283745, 31)\nClass distribution after cleaning:\n Class\n0    283253\n1       492\nName: count, dtype: int64\n\n\n\n#3.2 Drop irrelevant columns\n\n#Drop non-numeric column used for previous visualisations\nif 'Class_Label' in data.columns:\n    data.drop(columns=['Class_Label'], inplace=True)\n\n#Drop 'Time'\nif 'Time' in data.columns:\n    data.drop(columns=['Time'], inplace=True)\n\n\n#3.3 Split features and target\n\n#Feature/target split\nX = data.drop('Class', axis=1)\ny = data['Class']\n\n\n#3.4 Train-test split with stratification\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\nprint(\"Training shape:\", X_train.shape)\nprint(\"Test shape:\", X_test.shape)\nprint(\"Fraud in training set:\", y_train.sum())\nprint(\"Fraud in test set:\", y_test.sum())\n\nTraining shape: (198621, 29)\nTest shape: (85124, 29)\nFraud in training set: 344\nFraud in test set: 148\n\n\n\n# 3.5 Address class imbalance with SMOTE\n\n#Set fraud class to 30% of training data (vs 50% default)\nsmote = SMOTE(\n    sampling_strategy=0.1, \n    k_neighbors=2,          \n    random_state=42\n)\nX_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n\nprint(\"\\nAfter SMOTE:\")\nprint(\"Total training rows:\", X_train_sm.shape[0])\nprint(\"Fraud cases:\", y_train_sm.sum())\nprint(\"Genuine cases:\", len(y_train_sm) - y_train_sm.sum())\n\n\nAfter SMOTE:\nTotal training rows: 218104\nFraud cases: 19827\nGenuine cases: 198277\n\n\n\n# ---------------------------------------\n# 4. Model Training & Evaluation\n# ---------------------------------------\n\n#Helper to evaluate a model\n\ndef evaluate_model(name, y_true, y_pred, y_proba=None):\n    print(f\"\\n=== {name} Evaluation ===\")\n    print(confusion_matrix(y_true, y_pred))\n    print(classification_report(y_true, y_pred, digits=4))\n    if y_proba is not None:\n        auc = roc_auc_score(y_true, y_proba)\n        print(f\"ROC AUC: {auc:.4f}\")\n\n\n#4.1 Model 1 - Logistic Regression\n\n#Training Logistic Regression Model\nlog_reg = LogisticRegression(max_iter=1000, random_state=42)\nlog_reg.fit(X_train_sm, y_train_sm)\n\n#Predict on test set\ny_pred_log = log_reg.predict(X_test)\ny_proba_log = log_reg.predict_proba(X_test)[:, 1]\n\n#Evaluate Logistic Regression Performance\nevaluate_model('Logistic Regression', y_test, y_pred_log, y_proba_log)\n\n\n=== Logistic Regression Evaluation ===\n[[84870   106]\n [   31   117]]\n              precision    recall  f1-score   support\n\n           0     0.9996    0.9988    0.9992     84976\n           1     0.5247    0.7905    0.6307       148\n\n    accuracy                         0.9984     85124\n   macro avg     0.7621    0.8946    0.8150     85124\nweighted avg     0.9988    0.9984    0.9986     85124\n\nROC AUC: 0.9611\n\n\n\n#4.2 Model 2 - XGBoost\n\n#Training XGBoost Model\nxgb = XGBClassifier(\n    scale_pos_weight=10,  \n    max_depth=2,\n    learning_rate=0.05,\n    min_child_weight=5,\n    gamma=0.3,\n    subsample=0.7,\n    eval_metric='logloss',  \n    random_state=42\n)\nxgb.fit(X_train_sm, y_train_sm)  \n\n#Predict on test set\ny_proba_xgb = xgb.predict_proba(X_test)[:, 1]\ny_pred_xgb = (y_proba_xgb &gt; 0.9).astype(int)\n\n#Evaluate XGBoost Performance\nevaluate_model('XGBoost (0.9 threshold)', y_test, y_pred_xgb, y_proba_xgb)\n\n\n=== XGBoost (0.9 threshold) Evaluation ===\n[[84911    65]\n [   37   111]]\n              precision    recall  f1-score   support\n\n           0     0.9996    0.9992    0.9994     84976\n           1     0.6307    0.7500    0.6852       148\n\n    accuracy                         0.9988     85124\n   macro avg     0.8151    0.8746    0.8423     85124\nweighted avg     0.9989    0.9988    0.9989     85124\n\nROC AUC: 0.9645\n\n\n\n#4.3 Model 3 - GaussianNB\n\n#Training GaussianNB Model\nnb = GaussianNB()\nnb.fit(X_train_sm, y_train_sm)\n\n#Predict on test set\ny_pred_nb = nb.predict(X_test)\ny_proba_nb = nb.predict_proba(X_test)[:, 1]\n\n#Evaluate GaussianNB Performance\nevaluate_model('GaussianNB', y_test, y_pred_nb, y_proba_nb)\n\n\n=== GaussianNB Evaluation ===\n[[83053  1923]\n [   33   115]]\n              precision    recall  f1-score   support\n\n           0     0.9996    0.9774    0.9884     84976\n           1     0.0564    0.7770    0.1052       148\n\n    accuracy                         0.9770     85124\n   macro avg     0.5280    0.8772    0.5468     85124\nweighted avg     0.9980    0.9770    0.9868     85124\n\nROC AUC: 0.9410\n\n\n\n#4.4 Model 4 - Random Forest with GridSearchCV\n\n#Training Random Forest Model with Hyperparameter Tuning\n\n#Set Parameters\nparam_grid = {\n    'n_estimators': [100],\n    'max_depth': [3, 5],  \n    'min_samples_leaf': [5, 10],  \n    'max_features': [0.7, 'sqrt'], \n    'class_weight': [{0:1, 1:3}]  \n}\n#Setup GridSearchCV with 3 folds\ngrid_rf = GridSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    param_grid=param_grid,\n    scoring='recall',  \n    cv=3,              \n    n_jobs=-1,\n    verbose=0         \n)\n\n#Fit on SMOTE-balanced training set\ngrid_rf.fit(X_train_sm, y_train_sm)\n\n#Save the best tuned model\nbest_rf = grid_rf.best_estimator_\n\n#Print best parameters\nprint(\"Best RF Params:\", grid_rf.best_params_)\n\n#Predict on test set\ny_pred_rf = best_rf.predict(X_test)\ny_proba_rf = best_rf.predict_proba(X_test)[:, 1]\n\n#Evaluate Random Forest Performance \nevaluate_model('Random Forest', y_test, y_pred_rf, y_proba_rf)\n\nBest RF Params: {'class_weight': {0: 1, 1: 3}, 'max_depth': 5, 'max_features': 0.7, 'min_samples_leaf': 5, 'n_estimators': 100}\n\n=== Random Forest Evaluation ===\n[[84864   112]\n [   37   111]]\n              precision    recall  f1-score   support\n\n           0     0.9996    0.9987    0.9991     84976\n           1     0.4978    0.7500    0.5984       148\n\n    accuracy                         0.9982     85124\n   macro avg     0.7487    0.8743    0.7988     85124\nweighted avg     0.9987    0.9982    0.9984     85124\n\nROC AUC: 0.9546\n\n\n\n# ---------------------------------------\n# 5. Results Comparison\n# ---------------------------------------\n\n#Create Model Comparison Table\nresults = [\n    [\"Logistic Regression\", 0.525, 0.791, 106],\n    [\"Random Forest\", 0.498, 0.750, 112],\n    [\"XGBoost (0.9 threshold)\", 0.631, 0.750, 65],\n    [\"GaussianNB\", 0.056, 0.777, 1923]\n]\n\nprint(tabulate(results, \n              headers=[\"Model\", \"Precision\", \"Recall\", \"FP\"], \n              floatfmt=\".3f\",\n              tablefmt=\"github\"))\n\n| Model                   |   Precision |   Recall |   FP |\n|-------------------------|-------------|----------|------|\n| Logistic Regression     |       0.525 |    0.791 |  106 |\n| Random Forest           |       0.498 |    0.750 |  112 |\n| XGBoost (0.9 threshold) |       0.631 |    0.750 |   65 |\n| GaussianNB              |       0.056 |    0.777 | 1923 |\n\n\n\n#ROC Curve Comparison\n\n#Set up 2x3 ROC grid\nfig, axs = plt.subplots(2, 3, figsize=(18, 10))\n\n#Function to add baseline\ndef add_baseline(ax):\n    ax.plot([0, 1], [0, 1], linestyle='--', color='red')\n\n#Plot 1: Logistic Regression\nRocCurveDisplay.from_predictions(y_test, y_proba_log, ax=axs[0, 0], name='Logistic Regression')\naxs[0, 0].set_title(\"Logistic Regression\")\nadd_baseline(axs[0, 0])\n\n#Plot 2: XGBoost\nRocCurveDisplay.from_predictions(y_test, y_proba_xgb, ax=axs[0, 1], name='XGBoost')\naxs[0, 1].set_title(\"XGBoost\")\nadd_baseline(axs[0, 1])\n\n#Plot 3: GaussianNB\nRocCurveDisplay.from_predictions(y_test, y_proba_nb, ax=axs[0, 2], name='GaussianNB')\naxs[0, 2].set_title(\"GaussianNB\")\nadd_baseline(axs[0, 2])\n\n#Plot 4: Random Forest \nRocCurveDisplay.from_predictions(y_test, y_proba_rf, ax=axs[1, 0], name='Random Forest')\naxs[1, 0].set_title(\"Random Forest\")\nadd_baseline(axs[1, 0])\n\n#Plot 5: Combined ROC comparison \naxs[1, 1].set_title(\"Combined ROC Comparison\")\nRocCurveDisplay.from_predictions(y_test, y_proba_log, ax=axs[1, 1], name='Logistic')\nRocCurveDisplay.from_predictions(y_test, y_proba_xgb, ax=axs[1, 1], name='XGBoost')\nRocCurveDisplay.from_predictions(y_test, y_proba_nb, ax=axs[1, 1], name='GaussianNB')\nRocCurveDisplay.from_predictions(y_test, y_proba_rf, ax=axs[1, 1], name='Random Forest')\nadd_baseline(axs[1, 1])\n\n#Plot 6: Empty (for layout balance)\naxs[1, 2].axis('off')\n\n#Final touches\nplt.suptitle(\"ROC Curve Comparison Across All Models\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\n\n#Precision-Recall Curve Comparison\n\n#Set up 2x3 grid \nfig, axs = plt.subplots(2, 3, figsize=(18, 10))\n\n#Plot 1: Logistic Regression\nPrecisionRecallDisplay.from_predictions(y_test, y_proba_log, \n                                     ax=axs[0, 0], \n                                     name='Logistic Regression')\naxs[0, 0].set_title(\"Logistic Regression\")\naxs[0, 0].set_xlim([0, 1])  # Consistent axes\naxs[0, 0].set_ylim([0, 1])\n\n#Plot 2: XGBoost\nPrecisionRecallDisplay.from_predictions(y_test, y_proba_xgb, \n                                     ax=axs[0, 1], \n                                     name='XGBoost (0.9 threshold)')\naxs[0, 1].set_title(\"XGBoost\")\naxs[0, 1].set_xlim([0, 1])\naxs[0, 1].set_ylim([0, 1])\n\n#Plot 3: GaussianNB\nPrecisionRecallDisplay.from_predictions(y_test, y_proba_nb, \n                                     ax=axs[0, 2], \n                                     name='GaussianNB')\naxs[0, 2].set_title(\"GaussianNB\")\naxs[0, 2].set_xlim([0, 1])\naxs[0, 2].set_ylim([0, 1])\n\n#Plot 4: Random Forest\nPrecisionRecallDisplay.from_predictions(y_test, y_proba_rf, \n                                     ax=axs[1, 0], \n                                     name='Random Forest')\naxs[1, 0].set_title(\"Random Forest\")\naxs[1, 0].set_xlim([0, 1])\naxs[1, 0].set_ylim([0, 1])\n\n#Plot 5: Combined Comparison\naxs[1, 1].set_title(\"Combined Precision-Recall Comparison\")\nPrecisionRecallDisplay.from_predictions(y_test, y_proba_log, \n                                     ax=axs[1, 1], \n                                     name='Logistic')\nPrecisionRecallDisplay.from_predictions(y_test, y_proba_xgb, \n                                     ax=axs[1, 1], \n                                     name='XGBoost')\nPrecisionRecallDisplay.from_predictions(y_test, y_proba_nb, \n                                     ax=axs[1, 1], \n                                     name='GaussianNB')\nPrecisionRecallDisplay.from_predictions(y_test, y_proba_rf, \n                                     ax=axs[1, 1], \n                                     name='Random Forest')\n\n#Plot 6: Empty (for layout balance)\naxs[1, 2].axis('off')\n\n#Final touches\nplt.suptitle(\"Precision-Recall Curve Comparison Across All Models\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\n\n# ---------------------------------------\n# 6. Acknowledgements\n# ---------------------------------------\n\n# Kaggle Community Notebooks: for inspiring certain elements of the exploratory data analysis and visualisation:\n # - https://www.kaggle.com/code/marcinrutecki/smote-and-tomek-links-for-imbalanced-data\n # - https://www.kaggle.com/code/gargmanish/how-to-handle-imbalance-data-study-in-detail\n # - https://www.kaggle.com/code/marcinrutecki/best-techniques-and-metrics-for-imbalanced-dataset\n\n# Michael Farayola (Lecturer): Inspiration taken from the model evaluation approach, particularly the use of ROC visualisation:\n # - https://github.com/mmfara/python_application_project/blob/main/FRAUD%20DETECTION%20IN%20CREDIT%20CARD.ipynb\n\n# ChatGPT (OpenAI): Provided troubleshooting assistance during various code errors.\n\n# All external resources used were critically evaluated and adapted, ensuring originality and alignment with the learning outcomes of this module."
  },
  {
    "objectID": "projects/business_strat.html",
    "href": "projects/business_strat.html",
    "title": "Business Strategy",
    "section": "",
    "text": "Kerrygold Strategic Evaluation\nFor this year long module, we partook in a group project which accounted for 50% of our final grade. We had to pick a client company in which we would do a strategic evaluation of and propose 3 recommendations. Ornua, with a focus on their consumer brand Kerrygold is who we selected. As we dove into the strategic analyses, from PESTLE and Porter’s Five Forces to VRIO and value-chain mapping, I saw firsthand how each of our team member’s expertise added depth to the report, which ultimately earned us a grade of 80%. The report can be seen below. Additionally, we had to pitch our analysis, where myself and two others presented, acting as management consultants and we managed to score 78%! These great results set us up really nicely for the individual case analysis which accounted for the other 50% of our final grade.\n\n\n\n\nIndividual Case Study"
  },
  {
    "objectID": "projects/baa1027_adv_python.html",
    "href": "projects/baa1027_adv_python.html",
    "title": "Machine Learning & Advanced Python",
    "section": "",
    "text": "Code\n► See full ML algorithm code & outputs\n\n\nReport"
  },
  {
    "objectID": "projects/programming&visualisation.html",
    "href": "projects/programming&visualisation.html",
    "title": "Programming & Visualisation",
    "section": "",
    "text": "SQL\n\n\nWe started the semester out by learning about SQL. Throughout this learning I deepened my understanding of relational databases and SQL syntax, constructing ERDs, defining tables with proper primary and foreign keys, and mastering CRUD operations. Writing queries to join tables, calculate aggregates, and apply normalisation principles helped to sharpen my ability to think logically about data relationships and efficiencies. We had an exam on this which I achieved 89% in, which not only validated my proficiency with the core SQL concepts but also boosted my confidence to tackle complex data challenges in the future.\n► See SQL Exam Work\n\n\n\n\n\n\nPower BI\nBuilding on from our work on SQL, I then delved into learning Power BI.\n\n\nPython\n\n\nFinally, for the last 5 weeks of the semester, we explored the basics of Python."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Business Strategy\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning & Advanced Python\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Enterprise Development\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming & Visualisation\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "David Byrne",
    "section": "",
    "text": "Final Year Business Studies Student in DCU (Analytics Specialism), expected to graduate with first-class honours. I have a strong academic record, achieving 613 points in the Leaving Certificate and a 1.1 grade across the first and second years of the Business Studies curriculum. Gained invaluable experience in the Finance industry through a year-long internship with KKR & Co Inc. Complemented by certifications in SQL, Python, and financial modeling, I am poised to contribute effectively to an analytical role upon graduation in May 2025.\n\n\n\nCheck out some of my work\n► My projects can be seen here\n\n\n\nCore Skills\n\n\n\n&lt;i class=\"fa-brands fa-python skill-icon\"&gt;&lt;/i&gt;\n&lt;h3&gt;Python&lt;/h3&gt;\n&lt;p&gt;Built Python pipelines for data cleaning, visualization, and machine‑learning (pandas, scikit‑learn, XGBoost).&lt;/p&gt;\n&lt;a class=\"btn btn-primary\" href=\"projects/baa1027_adv_python.qmd\"&gt;→ View Python Projects&lt;/a&gt;\n\n\n\n&lt;i class=\"fa-solid fa-database skill-icon\"&gt;&lt;/i&gt;\n&lt;h3&gt;SQL&lt;/h3&gt;\n&lt;p&gt;Wrote complex SELECTs, JOINs, aggregations and performed normalization—89% in my SQL exam.&lt;/p&gt;\n&lt;a class=\"btn btn-primary\" href=\"projects/programming&visualisation.qmd\"&gt;→ View SQL Work&lt;/a&gt;\n\n\n\n&lt;i class=\"fa-solid fa-chart-line skill-icon\"&gt;&lt;/i&gt;\n&lt;h3&gt;Power BI&lt;/h3&gt;\n&lt;p&gt;Developed interactive dashboards with custom DAX metrics, filters, and user‑centred layouts.&lt;/p&gt;\n&lt;a class=\"btn btn-primary\" href=\"projects/programming&visualisation.qmd\"&gt;→ View Dashboards&lt;/a&gt;"
  },
  {
    "objectID": "about.html#core-skills",
    "href": "about.html#core-skills",
    "title": "David Byrne",
    "section": "Core Skills",
    "text": "Core Skills\n::: columns gutter=“4”\n\n\n\n\nPython\n\n\nBuilt Python pipelines for data cleaning, visualization, and machine‑learning (pandas, scikit‑learn, XGBoost).\n\n→ View Python Projects\n\n\n\n\n\n\nSQL\n\n\nWrote complex SELECTs, JOINs, aggregations and performed normalization—89% in my SQL exam.\n\n→ View SQL Work\n\n\n\n\n\n\nPower BI\n\n\nDeveloped interactive dashboards with custom DAX metrics, filters and user‑centred layouts.\n\n→ View Dashboards\n\n\n:::"
  }
]